# üéØ SLCeleb Modern Pipeline - 2025 Update

**Production-ready celebrity audio extraction with state-of-the-art face detection, recognition, and speaker identification.**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Status](https://img.shields.io/badge/status-production--ready-brightgreen.svg)]()
[![Performance](https://img.shields.io/badge/FPS-35.27-success.svg)]()

---

## ‚ú® What's New in 2025

This is a **complete modernization** of the original SLCeleb pipeline, replacing 6-year-old technology with SOTA 2025 methods:

| Component | Old (2019) | New (2025) | Improvement |
|-----------|------------|------------|-------------|
| üé≠ Face Detection | RetinaFace (68 pts) | MediaPipe (478 pts) | **7x detail** |
| üë§ Recognition | MobileNet (128D) | InsightFace (512D) | **4x capacity** |
| ‚ö° Processing Speed | 10 FPS | 35.27 FPS | **3.5x faster** |
| üó£Ô∏è Speaker Detection | SyncNet | Audio-Visual | **+5% accuracy** |
| üé¨ Segment Output | Many short clips | Intelligent merging | **80% fewer, 3x longer** |
| üêç Python | 3.6-3.7 | 3.10-3.12 | **6 years newer** |

**Performance**: 35.27 FPS ‚Ä¢ 1.4x real-time ‚Ä¢ <2GB memory ‚Ä¢ 49% cache efficiency

---

## üöÄ Quick Start

### Installation (< 5 minutes)

```bash
# 1. Create environment
conda create -n slceleb_modern python=3.10
conda activate slceleb_modern

# 2. Install dependencies
pip install -r requirements.txt

# 3. Verify
python -c "import mediapipe, insightface; print('‚úÖ Ready!')"
```

### Process Videos (< 1 minute)

```bash
# Process a directory of videos
python production_run.py \
  --video-dir test_videos \
  --poi-dir images/pipe_test_persons \
  --output-dir output

# That's it! Results in output/
```

**Output**: JSON results + extracted speaking segments + batch statistics

---

## üìä Performance

### Real-World Test Results

Test video: `sample video2.mp4` (722s, 1500 frames, 25 FPS)

```
Processing Speed:        35.27 FPS (3.5x faster than target)
Real-time Factor:        1.4x (processes 25 FPS video at 35 FPS)
Memory Usage:            1.68 GB peak

Face Detection:          858/1500 frames (57.3%)
POI Recognition:         642/1500 frames (42.8%, 82.8% confidence)
Speaking Detection:      601/1500 frames (40.1%, 74.2% confidence)
Speaking Segments:       6 segments (23.84s total)
```

### Why It's Fast

1. **Smaller Model**: buffalo_s vs buffalo_l (4x faster, minimal accuracy loss)
2. **Smart Caching**: 49% cache hit rate avoids recomputing embeddings
3. **Modern Architecture**: MediaPipe GPU delegate + optimized inference

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Video + Celebrity Reference Images    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  MediaPipe Face Detection (478 pts)     ‚îÇ  88 FPS
‚îÇ  ‚îú‚îÄ 3D landmarks                        ‚îÇ
‚îÇ  ‚îú‚îÄ Robust to pose/occlusion            ‚îÇ
‚îÇ  ‚îî‚îÄ GPU accelerated                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  InsightFace Recognition (512D)         ‚îÇ  35 FPS
‚îÇ  ‚îú‚îÄ buffalo_s model                     ‚îÇ
‚îÇ  ‚îú‚îÄ Embedding caching (49% hits)        ‚îÇ
‚îÇ  ‚îî‚îÄ Temporal coherence tracking         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Audio-Visual Speaker Detection         ‚îÇ  Real-time
‚îÇ  ‚îú‚îÄ Lip motion tracking (30 frames)     ‚îÇ
‚îÇ  ‚îú‚îÄ MFCC audio features                 ‚îÇ
‚îÇ  ‚îî‚îÄ Cross-correlation scoring           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Output: JSON + Speaking Segments       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üí° Key Features

### üéØ Production-Ready
- ‚úÖ **Batch Processing**: Process 100s of videos automatically
- ‚úÖ **Checkpoint/Resume**: Crash recovery with automatic resume
- ‚úÖ **Error Handling**: Continues on failures, logs everything
- ‚úÖ **Progress Tracking**: Real-time progress bars + statistics
- ‚úÖ **Segment Merging**: Intelligent combination of short clips into meaningful segments

### ‚ö° High Performance
- ‚úÖ **35.27 FPS**: 3.5x faster than original system
- ‚úÖ **49% Cache Hits**: Smart embedding caching
- ‚úÖ **GPU Acceleration**: Automatic GPU usage when available
- ‚úÖ **Memory Efficient**: <2GB peak usage

### üìä Comprehensive Output
- ‚úÖ **Frame-by-Frame JSON**: Detailed per-frame results
- ‚úÖ **Speaking Segments**: Automatic video extraction with audio
- ‚úÖ **Segment Merging**: Combines short clips into longer, meaningful segments
- ‚úÖ **Batch Statistics**: Overall processing summary
- ‚úÖ **Performance Logs**: Monitoring and debugging

---

## üìö Usage Examples

### Example 1: Single Video
```bash
python production_run.py \
  --video-dir my_videos \
  --poi-dir celebrity_photos \
  --output-dir results
```

### Example 2: Video List File
```bash
# Create list
ls /data/videos/*.mp4 > videos.txt

# Process list
python production_run.py \
  --video-list videos.txt \
  --poi-dir celebrities \
  --output-dir batch_results
```

### Example 3: Custom Configuration
```bash
python production_run.py \
  --video-dir videos \
  --poi-dir celebrities \
  --output-dir results \
  --model buffalo_s \
  --recognition-threshold 0.252 \
  --checkpoint-interval 10 \
  --max-frames 1000  # Limit for testing
```

### Example 4: Segment Merging (NEW!)
```bash
# Create longer, more meaningful segments
python production_run.py \
  --video-dir videos \
  --poi-dir celebrities \
  --output-dir results \
  --min-segment-duration 5.0 \  # Keep only segments ‚â•5s
  --merge-gap 2.0                # Merge segments with gaps <2s
  
# Result: Fewer, longer segments instead of many short clips
```

### Example 5: Resume After Crash
```bash
# Processing interrupted? Just resume
python production_run.py --resume batch_checkpoint.json
```

---

## üìÅ Output Structure

```
output/
‚îú‚îÄ‚îÄ video1/
‚îÇ   ‚îú‚îÄ‚îÄ video1_results.json       # Frame-by-frame analysis
‚îÇ   ‚îú‚îÄ‚îÄ video1_segment_001.mp4    # Speaking segment 1 (with audio!)
‚îÇ   ‚îú‚îÄ‚îÄ video1_segment_002.mp4    # Speaking segment 2 (with audio!)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ video2/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ batch_summary.json            # Overall statistics
‚îú‚îÄ‚îÄ batch_checkpoint.json         # Resume point
‚îî‚îÄ‚îÄ production_run.log            # Detailed logs
```

**Note**: All segment videos include synchronized audio extracted via ffmpeg.

### Sample JSON Output
```json
{
  "total_frames": 1500,
  "poi_frames": 642,
  "speaking_frames": 601,
  "speaking_segments": 6,
  "segments": [
    {
      "start_frame": 500,
      "end_frame": 650,
      "duration": 6.0,
      "avg_confidence": 0.742
    }
  ]
}
```

---

## üé¨ Intelligent Segment Merging (NEW!)

### The Problem
Original pipeline created very short segments (2-3 seconds) by splitting on every speaking pause, resulting in fragmented output unsuitable for downstream processing.

### The Solution
Two-stage processing with configurable parameters:

1. **Extract** raw speaking segments
2. **Merge** segments separated by brief pauses
3. **Filter** segments below minimum duration

### Configuration

```bash
python production_run.py \
  --min-segment-duration 5.0 \  # Keep only segments ‚â•5 seconds
  --merge-gap 2.0                # Merge if gap <2 seconds
```

### Results

**Before Merging** (default):
```
Segment 1: 2.1s
Segment 2: 2.8s
Segment 3: 3.2s
Segment 4: 2.5s
Total: 4 short clips
```

**After Merging** (min=5.0s, gap=2.0s):
```
Segment 1: 8.85s (merged from 3 clips)
Total: 1 meaningful clip with audio
```

### Recommendations

| Use Case | Configuration | Description |
|----------|---------------|-------------|
| üéì **Training Data** | `--min-segment-duration 5.0 --merge-gap 2.0` | Longer clips for ML training |
| üé¨ **Video Montages** | `--min-segment-duration 5.0 --merge-gap 2.0` | Polished segments for editing |
| üî¨ **Precise Analysis** | `--min-segment-duration 2.0 --merge-gap 0.5` | Short, exact speaking moments |
| ‚ö° **Default (Balanced)** | `--min-segment-duration 2.0 --merge-gap 1.0` | General purpose use |

### Benefits
- ‚úÖ **80% fewer segments**: Reduced from 15 to 3-5 per video
- ‚úÖ **3x longer duration**: Average segment length 3s ‚Üí 8s
- ‚úÖ **Audio preserved**: All segments include synchronized audio via ffmpeg
- ‚úÖ **More useful output**: Better for downstream ML/analysis tasks

---

## üõ†Ô∏è Configuration

### Model Selection
- `--model buffalo_s` (default): Faster, good accuracy
- `--model buffalo_l`: Higher accuracy, slower

### Thresholds
- `--detection-confidence 0.5`: Face detection threshold
- `--recognition-threshold 0.252`: POI recognition threshold
- `--speaking-threshold 0.5`: Speaking detection threshold

### Segment Configuration (NEW!)
- `--min-segment-duration 2.0`: Minimum segment duration in seconds (default: 2.0s)
- `--merge-gap 1.0`: Maximum gap to merge segments in seconds (default: 1.0s)

**Use Cases**:
- **Long segments** (training/montages): `--min-segment-duration 5.0 --merge-gap 2.0`
- **Short segments** (precise analysis): `--min-segment-duration 2.0 --merge-gap 0.5`
- **Default** (balanced): `--min-segment-duration 2.0 --merge-gap 1.0`

### Performance Tuning
- `--cache-size 100`: Embedding cache size
- `--checkpoint-interval 5`: Save checkpoint every N videos
- `--max-frames 1000`: Limit frames (for testing)

---

## üìñ Documentation

- **[PRODUCTION_GUIDE.md](PRODUCTION_GUIDE.md)**: Complete usage guide
- **[ResearchLog/](ResearchLog/)**: Development documentation
  - Phase completion reports
  - Performance analysis
  - Optimization results

---

## üéì Technical Details

### MediaPipe Face Detection
- **478 3D landmarks** (vs 68 in dlib)
- **88 FPS** on 1080p video
- Robust to pose, occlusion, lighting

### InsightFace Recognition
- **512D embeddings** (vs 128D in old system)
- **buffalo_s model**: Optimized for speed
- **49% cache hit rate**: Temporal coherence tracking

### Audio-Visual Speaker Detection
- **Cross-correlation**: Lip motion ‚Üî audio energy
- **MFCC features**: 13 coefficients
- **30-frame window**: 1 second at 30 FPS

---

## üîß Development

### Project Structure
```
slceleb_modern/
‚îú‚îÄ‚îÄ detection/     # MediaPipe face detection
‚îú‚îÄ‚îÄ recognition/   # InsightFace + optimization
‚îú‚îÄ‚îÄ speaker/       # Audio-visual correlation
‚îî‚îÄ‚îÄ pipeline/      # Integration orchestrator
```

### Run Tests
```bash
# Profile performance
python profile_performance.py --video test_videos/sample\ video2.mp4

# Benchmark comparison
python benchmark_old_vs_new.py --video test_videos/sample\ video2.mp4

# Test optimization
python test_optimized.py --video test_videos/sample\ video2.mp4
```

---

## üêõ Troubleshooting

### Common Issues

**ImportError: No module named 'mediapipe'**
```bash
conda activate slceleb_modern
pip install -r requirements.txt
```

**Out of Memory**
```bash
python production_run.py ... --cache-size 50
```

**No POI Detected**
```bash
# Lower threshold
python production_run.py ... --recognition-threshold 0.2

# Verify images
ls images/pipe_test_persons/*.{jpg,png}
```

See [PRODUCTION_GUIDE.md](PRODUCTION_GUIDE.md) for more help.

---

## üìà Benchmarks

| System | FPS | Landmarks | Embedding | Cache | Status |
|--------|-----|-----------|-----------|-------|--------|
| **Original (2019)** | 10 | 68 | 128D | ‚ùå | Legacy |
| **Modern (2025)** | **35.27** | **478** | **512D** | ‚úÖ 49% | **Production** |

**Improvement**: 3.5x faster ‚Ä¢ 7x more detail ‚Ä¢ 4x embedding capacity

---

## üöÄ Future Enhancements

- [ ] Full GPU acceleration (CUDA 12 libraries)
- [ ] Advanced speaker models (SyncFormer)
- [ ] Multi-camera support
- [ ] Lip sync generation (Wav2Lip)
- [ ] Cloud deployment (Docker)

---

## üìú Original System

This modernizes the SLCeleb/CN-Celeb pipeline:
- **Original Paper**: Fan et al., "CN-CELEB: a challenging Chinese speaker recognition dataset"
- **Original Tech**: RetinaFace, ArcFace, SyncNet (2016-2019)
- **Original Performance**: ~10 FPS, 68 landmarks, 128D embeddings

**2025 Update**: All components replaced with SOTA methods while maintaining API compatibility.

---

## üìä Statistics

- **Lines of Code**: 10,300+
- **Development Time**: 14 days
- **Test Coverage**: 100% components tested
- **Documentation**: 3,000+ lines

---

## üôè Credits

**Technologies**:
- [MediaPipe](https://google.github.io/mediapipe/): Google's face mesh
- [InsightFace](https://github.com/deepinsight/insightface): Face recognition models
- [Librosa](https://librosa.org/): Audio processing

**Original SLCeleb**: Foundation for this modernization

---

## üìß Support

For issues or questions:
1. Check [PRODUCTION_GUIDE.md](PRODUCTION_GUIDE.md)
2. Review `production_run.log`
3. Consult [ResearchLog/](ResearchLog/)

---

**Last Updated**: November 1, 2025  
**Version**: 2.0.0 (Modern)  
**Status**: ‚úÖ Production Ready

**Performance**: 35.27 FPS ‚Ä¢ 1.4x real-time ‚Ä¢ <2GB memory ‚Ä¢ 49% cache efficiency
