# Deep QA Report: slvideoprocess_2025

**Date:** October 31, 2025  
**Directory:** `/mnt/ricproject3/node5/SLCeleb_Videoprocess/slvideoprocess_2025`  
**Repository:** https://github.com/dimuthuanuraj/slvideoprocess_2025

---

## Executive Summary

This is a **Celebrity Audio Extraction Pipeline** based on the CN-Celeb project from Tsinghua University. It processes videos to automatically detect faces, recognize persons of interest (POI), validate speakers through lip-sync analysis, and extract labeled audio segments of specific celebrities.

**Overall Assessment:** ⚠️ **NOT READY TO RUN** - Multiple critical dependencies missing and configuration issues identified.

---

## 1. Application Purpose

The application performs multi-stage video processing for celebrity audio extraction:

1. **Face Detection** - Uses RetinaFace to detect faces in video frames
2. **Face Tracking** - Uses OpenCV trackers (MOSSE by default) to track detected faces
3. **Face Validation** - Uses InsightFace/FaceNet to verify if the face belongs to the target celebrity
4. **Speaker Validation** - Uses SyncNet to verify lip-audio synchronization (active speaker detection)
5. **Audio Extraction** - Extracts and labels audio segments where the celebrity is speaking

**Input:**
- Video files (MP4, etc.) organized by celebrity and category
- Reference images of the person of interest (POI)

**Output:**
- Text files with timestamps of when the POI is speaking (format: `HH:MM:SS:FF`)
- Optional: Processed video files with tracking overlays

---

## 2. How to Run the Application

### 2.1 Main Entry Points

#### Option 1: Batch Processing (Primary Method)
```bash
python run.py --POI "<celebrity_name>"
```

**Requirements:**
- Configure paths in `common.py`:
  - `video_base_dir`: Base directory containing videos
  - `image_base_dir`: Base directory containing POI reference images
  - `output_dir`: Where to save results
  - `temp_dir` and `log_dir`: Temporary and log directories

**Directory Structure Expected:**
```
video_base_dir/
  ├── Celebrity1/
  │   ├── interview/
  │   │   └── video1.mp4
  │   ├── speech/
  │   └── entertainment/
  └── Celebrity2/
      └── ...

image_base_dir/
  ├── Celebrity1/
  │   ├── photo1.jpg
  │   ├── photo2.jpg
  │   └── ...
  └── Celebrity2/
      └── ...
```

#### Option 2: Single Video Processing
```bash
python run_single.py
```

**Note:** This file needs modification as it contains hardcoded Windows paths:
```python
# Current (hardcoded):
POI_imgs = [os.path.join('images/蔡依林', pic) for pic in os.listdir('images/蔡依林')]
video_dir = r'C:\Users\haoli\Desktop\video\蔡依林\song\song-1.mp4'
```

### 2.2 Configuration

Edit `common.py` to configure:

```python
class Config:
    # Critical paths (MUST BE CONFIGURED)
    video_base_dir = "C:/Users/haoli/Desktop/videos"  # ⚠️ Change this
    image_base_dir = "./images"                        # ⚠️ Change this
    output_dir = "./result"
    
    # Model paths (should work if models are present)
    landmark_predictor = "model/dlib/shape_predictor_68_face_landmarks.dat"
    retinaface_model = 'model/retinaface_model/mnet.25/mnet.25'
    syncnet_model = "./model/syncnet_v2.model"
    mobilenet_dir = './model/insightface_model/mobilenet/model,0'
    
    # Processing options
    use_facenet = False        # Use InsightFace (default) or FaceNet
    enable_syncnet = True      # Enable speaker validation
    debug = False              # Show debug info
    showimg = False            # Display video frames during processing
    write_video = False        # Write output video with overlays
    
    # Thresholds
    starting_confidence = 4    # SyncNet starting threshold
    patient_confidence = 3     # SyncNet continuation threshold
    cosine_threshold = 0.8     # Face validation threshold
```

---

## 3. Critical Issues Identified

### 3.1 Missing Python Dependencies ❌

**Missing Core Libraries:**
```
✗ opencv-python (cv2) - CRITICAL
✗ tensorflow - CRITICAL (if use_facenet=True)
✗ keras - CRITICAL (if use_facenet=True)
✗ mxnet - CRITICAL (required for InsightFace/RetinaFace)
✗ dlib - CRITICAL
✗ python-speech-features - CRITICAL
```

**Installed:**
```
✓ torch (2.8.0)
✓ scipy (1.13.1)
✓ scikit-learn (1.5.1)
```

### 3.2 Missing System Dependencies ❌

```
✗ ffmpeg - CRITICAL (required for audio extraction)
```

The application uses ffmpeg to extract audio:
```python
command = ("ffmpeg -y -i %s -async 1 -ac 1 -vn -acodec pcm_s16le -ar 16000 %s > %s 2>&1" % 
           (video_dir, audio_tmp, os.path.join(config.log_dir, "ffmpeg.log")))
```

### 3.3 Platform-Specific Issues ⚠️

**Windows Paths in Configuration:**
- `common.py` contains Windows paths: `C:/Users/haoli/Desktop/videos`
- `run_single.py` has hardcoded Windows paths with Chinese characters

**Platform Detection:**
```python
# face_detection.py
if platform.platform().find('Windows') >= 0:
    from RetinaFace.insightface.RetinaFace.retinaface import RetinaFace
elif platform.platform().find('Linux') >= 0:
    from RetinaFace.insightface.RetinaFace_linux.retinaface import RetinaFace
```

### 3.4 Code Quality Issues ⚠️

**1. Hardcoded Python Version Compatibility:**
- Uses deprecated `scipy.misc.imread` (removed in scipy 1.3+)
- Should use `imageio.imread` or `PIL.Image.open` instead

**2. Deprecated OpenCV Trackers:**
```python
# cv_tracker.py - These methods are deprecated in newer OpenCV versions
self.tracker = cv2.TrackerMOSSE_create()  # Now: cv2.legacy.TrackerMOSSE_create()
```

**3. Framework Version Conflicts:**
- PyTorch 2.8.0 is installed, but code expects older API
- `torch.autograd.Variable` is deprecated (no longer needed in PyTorch 2.x)
```python
# SyncNet/SyncNetInstance.py
imtv = torch.autograd.Variable(torch.from_numpy(im.astype(float)).float())
# Should be: imtv = torch.from_numpy(im.astype(float)).float()
```

**4. Mixed Framework Dependencies:**
- Requires both TensorFlow (for FaceNet) AND MXNet (for InsightFace/RetinaFace)
- Also requires PyTorch (for SyncNet)
- This is a heavy dependency footprint

**5. Video FPS Constraint:**
```python
# run.py line 90
if config.enable_syncnet:
    assert video_fps == 25  # ⚠️ Only works with 25 FPS videos
```

**6. Incomplete Requirements File:**
- Only `Speaker-Diarization/requirements.txt` exists (numpy, scipy, torch)
- No main requirements.txt with all dependencies

### 3.5 Model Files Status ✓

**Good News - Models Present:**
```
✓ model/syncnet_v2.model (53MB) - Present
✓ model/dlib/shape_predictor_68_face_landmarks.dat (96MB) - Present
✓ model/retinaface_model/mnet.25/ - Present
✓ model/insightface_model/mobilenet/ - Present
```

### 3.6 Configuration Issues ⚠️

**1. No Input Validation:**
- Doesn't check if video/image directories exist before processing
- Will crash if paths are invalid

**2. Memory Management:**
- Uses `gc.collect()` after each video but no explicit memory limits
- Could run out of memory on large video batches

**3. No Progress Persistence:**
- If processing crashes, must restart from beginning
- No checkpoint/resume functionality

---

## 4. Dependency Installation Guide

### 4.1 System Dependencies

```bash
# Install ffmpeg
sudo apt-get update
sudo apt-get install -y ffmpeg

# Install system libraries for OpenCV
sudo apt-get install -y libgl1-mesa-glx libglib2.0-0
```

### 4.2 Python Dependencies

**Create a comprehensive requirements.txt:**

```txt
# Core Libraries
numpy>=1.15.1
scipy>=1.1.0,<1.3.0  # scipy.misc.imread removed in 1.3+
scikit-learn>=1.5.0

# Deep Learning Frameworks
torch>=1.0.0,<2.0.0  # Code uses deprecated APIs
mxnet>=1.5.0
tensorflow>=1.12.0,<2.0.0  # If using FaceNet
keras>=2.2.4  # If using FaceNet

# Computer Vision
opencv-python>=4.5.0
opencv-contrib-python>=4.5.0  # For tracking algorithms
dlib>=19.18.0

# Audio Processing
python-speech-features>=0.6

# Utilities
pandas>=0.24.0
imageio>=2.5.0  # To replace scipy.misc.imread
Pillow>=6.0.0
```

**Installation:**
```bash
cd /mnt/ricproject3/node5/SLCeleb_Videoprocess/slvideoprocess_2025

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

**Alternative (Conda):**
```bash
conda create -n slceleb python=3.7
conda activate slceleb
conda install pytorch torchvision -c pytorch
conda install -c conda-forge opencv dlib ffmpeg
pip install mxnet python-speech-features
```

---

## 5. Required Code Fixes

### 5.1 Fix scipy.misc.imread (CRITICAL)

**File:** `face_validation.py` (line ~85)

**Current:**
```python
from scipy import misc
img = misc.imread(os.path.expanduser(image), mode='RGB')
```

**Fixed:**
```python
from PIL import Image
import numpy as np
img = np.array(Image.open(os.path.expanduser(image)).convert('RGB'))
```

### 5.2 Fix OpenCV Tracker API

**File:** `cv_tracker.py` (lines 9-23)

**Current:**
```python
if config.tracker_type == 'MOSSE':
    self.tracker = cv2.TrackerMOSSE_create()
```

**Fixed:**
```python
if config.tracker_type == 'MOSSE':
    self.tracker = cv2.legacy.TrackerMOSSE_create()
```

Apply to all tracker types.

### 5.3 Fix PyTorch Variable (Optional)

**File:** `SyncNet/SyncNetInstance.py`

**Current:**
```python
imtv = torch.autograd.Variable(torch.from_numpy(im.astype(float)).float())
```

**Fixed:**
```python
imtv = torch.from_numpy(im.astype(float)).float()
```

### 5.4 Fix Configuration Paths

**File:** `common.py`

**Current:**
```python
video_base_dir = "C:/Users/haoli/Desktop/videos"
```

**Fixed:**
```python
video_base_dir = "/mnt/ricproject3/node5/SLCeleb_Videoprocess/videos"  # Or your path
```

---

## 6. Testing Recommendations

### 6.1 Minimal Test

1. Create test structure:
```bash
mkdir -p images/test_person
mkdir -p videos/test_person/interview
mkdir -p result temp log
```

2. Add test images and video

3. Update `common.py`:
```python
video_base_dir = "/mnt/ricproject3/node5/SLCeleb_Videoprocess/videos"
image_base_dir = "/mnt/ricproject3/node5/SLCeleb_Videoprocess/images"
```

4. Run:
```bash
python run.py --POI "test_person"
```

### 6.2 Debug Mode

Enable debug mode in `common.py`:
```python
debug = True
showimg = True  # If you have X11 forwarding
```

---

## 7. Performance Considerations

### 7.1 Resource Requirements

**Estimated Requirements:**
- **RAM:** 8-16GB (depends on video resolution and batch size)
- **GPU:** Recommended (CUDA-capable, configured in `config.gpuid`)
- **Disk:** Depends on video size (temp files created)
- **Processing Time:** ~1-5 FPS on CPU, 10-25 FPS on GPU

### 7.2 Bottlenecks

1. **Face Detection:** RetinaFace is relatively fast
2. **Face Recognition:** InsightFace is efficient
3. **SyncNet:** Most compute-intensive (requires lip region extraction and audio processing)
4. **I/O:** Audio extraction via ffmpeg can be slow

---

## 8. Security & Privacy Concerns

⚠️ **Sensitive Application:**
- Processes biometric data (faces, voice)
- Ensure compliance with privacy regulations (GDPR, etc.)
- Consider data retention policies
- Secure storage of POI images and extracted audio

---

## 9. Recommendations

### 9.1 Immediate Actions

1. ✅ Install all missing dependencies (see Section 4)
2. ✅ Fix critical code issues (scipy.misc, cv2 trackers)
3. ✅ Update configuration paths to Linux paths
4. ✅ Create proper requirements.txt at project root
5. ✅ Test with a single small video first

### 9.2 Short-term Improvements

1. Add command-line argument parsing to override config paths
2. Add input validation and better error handling
3. Implement checkpoint/resume functionality
4. Add progress bars (tqdm)
5. Create Docker container for easier deployment
6. Add logging instead of print statements

### 9.3 Long-term Improvements

1. Modernize PyTorch code (remove Variable)
2. Support variable FPS videos (not just 25 FPS)
3. Add unit tests
4. Refactor to remove hardcoded Chinese strings
5. Consider migrating from Python 2/3 compatibility code
6. Add CI/CD pipeline
7. Create proper documentation
8. Add GPU memory management

---

## 10. Summary Checklist

### Before Running:

- [ ] Install ffmpeg
- [ ] Install all Python dependencies (opencv, mxnet, dlib, etc.)
- [ ] Fix scipy.misc.imread in face_validation.py
- [ ] Fix cv2 tracker API in cv_tracker.py
- [ ] Update paths in common.py
- [ ] Create input directory structure
- [ ] Add POI images to images/ directory
- [ ] Add videos to videos/ directory
- [ ] Verify model files are present (they are ✓)
- [ ] Create output/temp/log directories
- [ ] Test GPU availability if using GPU

### After Setup:

- [ ] Run test with single video
- [ ] Verify output format
- [ ] Check logs for errors
- [ ] Monitor resource usage
- [ ] Validate results

---

## Conclusion

This is a **complex multi-model video processing pipeline** that requires significant setup. The code is functional but **outdated** and has **multiple dependency issues**. With the fixes outlined above, it should work for celebrity audio extraction tasks.

**Estimated Setup Time:** 2-4 hours  
**Risk Level:** Medium (due to deprecated APIs and missing dependencies)  
**Code Maturity:** Research/Academic (not production-ready)

**Recommendation:** If you need to use this, allocate time for dependency resolution and testing. Consider containerization (Docker) for easier deployment and reproducibility.
